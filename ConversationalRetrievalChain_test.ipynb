{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcfd3e6",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, we follow the tutorial of LangChian ConversationalRetrievalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31586c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36caa5de",
   "metadata": {},
   "source": [
    "### 1. Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36875aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_name = 'Physics-Informed Neural Operator for Learning Partial Differential Equations'\n",
    "paper_name = 'Learning the solution operator of parametric partial differential equations with physics-informed DeepOnets'\n",
    "# paper_name = 'Sex-specific and opposed effects of FKBP51 in glutamatergic and GABAergic neurons_Implications for stress susceptibility and resilience'\n",
    "loader = PyMuPDFLoader(\"./Papers/\"+paper_name+\".pdf\")\n",
    "documents = loader.load()\n",
    "vectorstore_path = \"./\"+paper_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f07cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "text = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eaeeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28074844",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.split_text(text[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6810e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [src.page_content for src in source]\n",
    "phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI settings\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_base = \"https://abb-chcrc.openai.azure.com/\"  # Your Azure OpenAI resource's endpoint value.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", \n",
    "                              deployment=\"text-embedding-ada-002\",\n",
    "                              openai_api_base=\"https://abb-chcrc.openai.azure.com/\",\n",
    "                              openai_api_type=\"azure\",\n",
    "                              chunk_size=1)\n",
    "\n",
    "# if not os.path.exists(vectorstore_path) and os.path.isdir(vectorstore_path):\n",
    "if not os.path.exists(vectorstore_path):\n",
    "    print(\"Embeddings not found! Creating new ones\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    vectorstore.save_local(vectorstore_path)\n",
    "else:\n",
    "    print(\"Embeddings found! Loaded the computed ones\")\n",
    "    vectorstore = FAISS.load_local(vectorstore_path, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d46a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c9541",
   "metadata": {},
   "source": [
    "### 2. Summary of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007285b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "llm = AzureChatOpenAI(openai_api_base=\"https://abb-chcrc.openai.azure.com/\",\n",
    "                    openai_api_version=\"2023-03-15-preview\",\n",
    "                    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                    openai_api_type=\"azure\",\n",
    "                    deployment_name=\"gpt-35-turbo-0301\",\n",
    "                    temperature=0.7)\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "summary = chain.run(documents[:2])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e65552",
   "metadata": {},
   "source": [
    "**For arXiv papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e955df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import ArxivAPIWrapper\n",
    "\n",
    "# Retrieve paper metadata\n",
    "paper_arxiv_id = '2103.10974'\n",
    "arxiv = ArxivAPIWrapper()\n",
    "summary = arxiv.run(paper_arxiv_id)\n",
    "\n",
    "# String manipulation\n",
    "summary = summary.replace('{', '(').replace('}', ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bf2a9",
   "metadata": {},
   "source": [
    "### 3. Journalist bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JournalistBot:\n",
    "    \"\"\"Class definition for the journalist bot, created with LangChain.\"\"\"\n",
    "    \n",
    "    def __init__(self, engine):\n",
    "        \"\"\"Select backbone large language model, as well as instantiate \n",
    "        the memory for creating language chain in LangChain.\n",
    "        \n",
    "        Args:\n",
    "        --------------\n",
    "        engine: the backbone llm-based chat model.\n",
    "                \"OpenAI\" stands for OpenAI chat model;\n",
    "                Other chat models are also possible in LangChain, \n",
    "                see https://python.langchain.com/en/latest/modules/models/chat/integrations.html\n",
    "        \"\"\"\n",
    "        \n",
    "        # Instantiate llm\n",
    "        if engine == 'OpenAI':\n",
    "            self.llm = ChatOpenAI(\n",
    "                model_name=\"gpt-3.5-turbo\",\n",
    "                temperature=0.8\n",
    "            )\n",
    "            \n",
    "        elif engine == 'Azure':\n",
    "            self.llm = AzureChatOpenAI(\n",
    "            openai_api_base=\"https://abb-chcrc.openai.azure.com/\",\n",
    "            openai_api_version=\"2023-03-15-preview\",\n",
    "            openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "            openai_api_type=\"azure\",\n",
    "            deployment_name=\"gpt-35-turbo-0301\",\n",
    "            temperature=0.8)\n",
    "\n",
    "        else:\n",
    "            raise KeyError(\"Currently unsupported chat model type!\")\n",
    "        \n",
    "        # Instantiate memory\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "\n",
    "    def instruct(self, topic, abstract):\n",
    "        \"\"\"Determine the context of chatbot interaction. \n",
    "        \n",
    "        Args:\n",
    "        -----------    \n",
    "        \"\"\"\n",
    "        \n",
    "        self.topic = topic\n",
    "        self.abstract = abstract\n",
    "        \n",
    "        # Define prompt template\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(self._specify_system_message()),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"\"\"{input}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        # Create conversation chain\n",
    "        self.conversation = ConversationChain(memory=self.memory, prompt=prompt, \n",
    "                                              llm=self.llm, verbose=False)\n",
    "        \n",
    "\n",
    "    def step(self, prompt):\n",
    "        response = self.conversation.predict(input=prompt)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "\n",
    "    def _specify_system_message(self):\n",
    "        \"\"\"Specify the behavior of the journalist chatbot.\n",
    "\n",
    "\n",
    "        Outputs:\n",
    "        --------\n",
    "        prompt: instructions for the chatbot.\n",
    "        \"\"\"       \n",
    "        \n",
    "        # Compile bot instructions \n",
    "        prompt = f\"\"\"You are a technical journalist interested in {self.topic}, \n",
    "        Your task is to distill a recently published scientific paper on this topic through\n",
    "        an interview with the author, which is played by another chatbot.\n",
    "        Your objective is to ask comprehensive and technical questions \n",
    "        so that anyone who reads the interview can understand the paper's main ideas and contributions, \n",
    "        even without reading the paper itself. \n",
    "        You're provided with the paper's summary to guide your initial questions.\n",
    "        You must keep the following guidelines in mind:\n",
    "        - Focus exclusive on the technical content of the paper.\n",
    "        - Avoid general questions about {self.topic}, focusing instead on specifics related to the paper.\n",
    "        - Only ask one question at a time.\n",
    "        - Feel free to ask about the study's purpose, methods, results, and significance, \n",
    "        and clarify any technical terms or complex concepts. \n",
    "        - Your goal is to lead the conversation towards a clear and engaging summary.\n",
    "        - Do not include any prefixed labels like \"Interviewer:\" or \"Question:\" in your question.\n",
    "        \n",
    "        [Abstract]: {self.abstract}\"\"\"\n",
    "        \n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579dded3",
   "metadata": {},
   "source": [
    "### 4. Author bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorBot:\n",
    "    \"\"\"Class definition for the author bot, created with LangChain.\"\"\"\n",
    "    \n",
    "    def __init__(self, engine, vectorstore):\n",
    "        \"\"\"Select backbone large language model, as well as instantiate \n",
    "        the memory for creating language chain in LangChain.\n",
    "        \n",
    "        Args:\n",
    "        --------------\n",
    "        engine: the backbone llm-based chat model.\n",
    "                \"OpenAI\" stands for OpenAI chat model;\n",
    "                Other chat models are also possible in LangChain, \n",
    "                see https://python.langchain.com/en/latest/modules/models/chat/integrations.html\n",
    "        \"\"\"\n",
    "        \n",
    "        # Instantiate llm\n",
    "        if engine == 'OpenAI':\n",
    "            self.llm = ChatOpenAI(\n",
    "                model_name=\"gpt-3.5-turbo\",\n",
    "                temperature=0.6\n",
    "            )\n",
    "            \n",
    "        elif engine == 'Azure':\n",
    "            self.llm = AzureChatOpenAI(\n",
    "            openai_api_base=\"https://abb-chcrc.openai.azure.com/\",\n",
    "            openai_api_version=\"2023-03-15-preview\",\n",
    "            openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "            openai_api_type=\"azure\",\n",
    "            deployment_name=\"gpt-35-turbo-0301\",\n",
    "            temperature=0.6)\n",
    "\n",
    "        else:\n",
    "            raise KeyError(\"Currently unsupported chat model type!\")\n",
    "        \n",
    "        # Instantiate memory\n",
    "#         self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) \n",
    "        self.chat_history = []\n",
    "        \n",
    "        # Instantiate embedding index\n",
    "        self.vectorstore = vectorstore\n",
    "        \n",
    "        \n",
    "        \n",
    "    def instruct(self, topic):\n",
    "        \"\"\"Determine the context of chatbot interaction. \n",
    "        \n",
    "        Args:\n",
    "        -----------    \n",
    "        \"\"\"\n",
    "        # Specify topic\n",
    "        self.topic = topic\n",
    "        \n",
    "        general_system_template = r\"\"\" \n",
    "        Given a specific context, please give a short answer to the question, covering the required advices in general and then provide the names all of relevant(even if it relates a bit) products. \n",
    "         ----\n",
    "        {context}\n",
    "        ----\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define prompt template\n",
    "        qa_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "        ])\n",
    "        \n",
    "        # Create conversation chain\n",
    "        self.conversation_qa = ConversationalRetrievalChain.from_llm(llm=self.llm, \n",
    "                                                                     retriever=self.vectorstore.as_retriever(\n",
    "                                                                         search_kwargs={\"k\": 3}),\n",
    "                                                                    chain_type=\"stuff\", return_source_documents=True,\n",
    "                                                                    combine_docs_chain_kwargs={'prompt': qa_prompt})\n",
    "\n",
    "        \n",
    "        \n",
    "    def step(self, prompt):\n",
    "        response = self.conversation_qa({\"question\": prompt, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.append((prompt, response[\"answer\"]))\n",
    "        \n",
    "        return response[\"answer\"], response[\"source_documents\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _specify_system_message(self):\n",
    "        \"\"\"Specify the behavior of the author chatbot.\n",
    "\n",
    "\n",
    "        Outputs:\n",
    "        --------\n",
    "        prompt: instructions for the chatbot.\n",
    "        \"\"\"       \n",
    "        \n",
    "        # Compile bot instructions \n",
    "        prompt = f\"\"\"You are the author of a recently published scientific paper on {self.topic}.\n",
    "        You are being interviewed by a technical journalist who is played by another chatbot and\n",
    "        looking to write an article to summarize your paper.\n",
    "        Your task is to provide comprehensive, clear, and accurate answers to the journalist's questions.\n",
    "        Please keep the following guidelines in mind:\n",
    "        - Try to explain complex concepts and technical terms in an understandable way, without sacrificing accuracy.\n",
    "        - Your responses should primarily come from the relevant content of this paper, \n",
    "        which will be provided to you in the following, but you can also use your broad knowledge in {self.topic} to \n",
    "        provide context or clarify complex topics. \n",
    "        - Remember to differentiate when you are providing information directly from the paper versus \n",
    "        when you're giving additional context or interpretation. Use phrases like 'According to the paper...' for direct information, \n",
    "        and 'Based on general knowledge in the field...' when you're providing additional context.\n",
    "        - Only answer one question at a time. Ensure that each answer is complete before moving on to the next question.\n",
    "        - Do not include any prefixed labels like \"Author:\", \"Interviewee:\", Respond:\", or \"Answer:\" in your answer.\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt += \"\"\"Given the following context, please answer the question.\n",
    "        \n",
    "        {context}\"\"\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d31f1",
   "metadata": {},
   "source": [
    "### 5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a9ff8",
   "metadata": {},
   "source": [
    "Standalone test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39587ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "            openai_api_base=\"https://abb-chcrc.openai.azure.com/\",\n",
    "            openai_api_version=\"2023-03-15-preview\",\n",
    "            openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "            openai_api_type=\"azure\",\n",
    "            deployment_name=\"gpt-35-turbo-0301\",\n",
    "            temperature=0.7)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) \n",
    "topic = 'physics-informed machine learning'\n",
    "\n",
    "prompt = f\"\"\"You are an expert in {topic} and \n",
    "your primary role is to provide detailed answers to questions asked by a 'journalist bot'. \n",
    "You should help the journalist bot to understand the key points of your paper.\n",
    "Try to explain complex concepts and technical terms in an understandable way, without sacrificing accuracy.\n",
    "Your responses should primarily come from the relevant content of this paper, \n",
    "which will be provided to you in the following, but you can also use your broad knowledge in {topic} to \n",
    "provide context or clarify complex topics. Remember to differentiate when you are providing information \n",
    "directly from the paper versus when you're giving additional context or interpretation. \n",
    "Use phrases like 'According to the paper...' for direct information, \n",
    "and 'Based on general knowledge in the field...' when you're providing additional context.\"\"\"\n",
    "\n",
    "prompt += \"\"\"Given the following context, please answer the question.\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44080edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_qa = ConversationalRetrievalChain.from_llm(llm=llm, verbose=False,\n",
    "                                                        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                                                        chain_type=\"stuff\", return_source_documents=True,\n",
    "                                                        combine_docs_chain_kwargs={'prompt': qa_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb46f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "result = conversation_qa({\"question\": question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abda588",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca56344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['source_documents'][0].__dict__['page_content'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['source_documents'][0].__dict__['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7102b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1459d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversation_qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2eaaeb",
   "metadata": {},
   "source": [
    "Integration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddab211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate journalist and author bot\n",
    "journalist = JournalistBot('Azure')\n",
    "author = AuthorBot('Azure', vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f18dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide instruction\n",
    "journalist.instruct(topic='physics-informed machine learning', abstract=summary)\n",
    "author.instruct('physics-informed machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e25843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation\n",
    "question_hist = []\n",
    "answer_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4967364",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = journalist.step('Start the conversation')\n",
    "print(question)\n",
    "\n",
    "question_hist.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f1a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, source = author.step(question)\n",
    "print(answer)\n",
    "\n",
    "answer_list.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = journalist.step(answer)\n",
    "print(question)\n",
    "\n",
    "question_hist.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, source = author.step(question)\n",
    "print(answer)\n",
    "\n",
    "answer_list.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d2b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def highlight_text(file_path, phrases, output_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    \n",
    "    for page in doc:\n",
    "        for phrase in phrases:            \n",
    "            text_instances = page.search_for(phrase)\n",
    "\n",
    "            for inst in text_instances:\n",
    "                highlight = page.add_highlight_annot(inst)\n",
    "    \n",
    "    doc.save(output_path, garbage=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea29224",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = 'Learning the solution operator of parametric partial differential equations with physics-informed DeepOnets'\n",
    "paper_path = \"./Papers/\"+paper+\".pdf\"\n",
    "phrases = [src.page_content for src in source]\n",
    "page_numbers = [str(src.metadata['page']+1) for src in source]\n",
    "\n",
    "highlight_text(paper_path, phrases, \"highlighted.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee7b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0].search_for(phrases[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f243a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f955c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e058f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0].get_text(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6904fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5709b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0].get_text(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "doc = fitz.open(paper_path)\n",
    "\n",
    "for page in doc:\n",
    "    for phrase in phrases:            \n",
    "        text_instances = page.search_for(phrase)\n",
    "        print(text_instances)\n",
    "\n",
    "        for inst in text_instances:\n",
    "            highlight = page.add_highlight_annot(inst)\n",
    "\n",
    "# doc.save(output_path, garbage=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8a1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4d2dd",
   "metadata": {},
   "source": [
    "Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source[0].metadata['page']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89688099",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = f\"\"\"For details, check: \\n\\n\"\"\"\n",
    "for doc in source[:2]:\n",
    "    string += f\"page {doc.metadata['page']+1}, start with '{doc.page_content[:100]}'\\n \\n\"\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b4e53",
   "metadata": {},
   "source": [
    "User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate journalist and author bot\n",
    "journalist = JournalistBot('Azure')\n",
    "author = AuthorBot('Azure', vectorstore)\n",
    "\n",
    "# Provide instruction\n",
    "journalist.instruct(topic='neuroscience stress-related disorder FKBP51', abstract=summary)\n",
    "author.instruct('neuroscience stress-related disorder FKBP51')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a text area for the chat log\n",
    "chat_log = widgets.HTML(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create a text input field for user input\n",
    "user_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Question',\n",
    "    description='',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"60%\")\n",
    ")\n",
    "\n",
    "# Create buttons for continue and send\n",
    "bot_button = widgets.Button(description=\"Bot ask\")\n",
    "user_button = widgets.Button(description=\"User ask\")\n",
    "\n",
    "# Define the button click callbacks\n",
    "def on_bot_button_clicked(b):\n",
    "    if chat_log.value == '':\n",
    "        # The conversation is just starting\n",
    "        bot_question = journalist.step(\"Start the conversation\")\n",
    "    else:\n",
    "        # The conversation is ongoing, generate a question based on the last response from author_bot\n",
    "        bot_question = journalist.step(chat_log.value.split(\"<br><br>\")[-1])\n",
    "    \n",
    "    chat_log.value += \"<br><b style='color:blue'>Journalist Bot:</b> \" + bot_question\n",
    "    \n",
    "    # Author bot responds to the question\n",
    "    bot_response = author.step(bot_question)\n",
    "    chat_log.value += \"<br><br><b style='color:green'>Author Bot:</b> \" + bot_response + \"<br>\"\n",
    "\n",
    "    \n",
    "def on_user_button_clicked(b):\n",
    "    # User asks a question\n",
    "    bot_response = author.step(user_input.value)\n",
    "    chat_log.value += \"<br><br><b style='color:purple'>You:</b> \" + user_input.value\n",
    "    chat_log.value += \"<br><br><b style='color:green'>Author Bot:</b> \" + bot_response + \"<br>\"\n",
    "    \n",
    "    # Inform journalist bot about the asked questions \n",
    "    journalist.memory.chat_memory.add_user_message(user_input.value)\n",
    "    \n",
    "    # Clear user input\n",
    "    user_input.value = \"\"\n",
    "\n",
    "# Attach the callbacks\n",
    "bot_button.on_click(on_bot_button_clicked)\n",
    "user_button.on_click(on_user_button_clicked)\n",
    "\n",
    "# Use HBox and VBox for arranging the widgets\n",
    "first_row = widgets.HBox([bot_button])\n",
    "second_row = widgets.HBox([user_button, user_input])\n",
    "\n",
    "# Display the UI\n",
    "display(chat_log, widgets.VBox([first_row, second_row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95db029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
